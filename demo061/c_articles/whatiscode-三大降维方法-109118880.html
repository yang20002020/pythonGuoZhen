<head> <meta charset="utf-8" </head>
                      <div id="content_views" class="htmledit_views">
                    <article class="post_article" style="font-size: 16px;"> 
 <h4>PCA(Principal Component Analysis, 主成分分析)</h4> 
 <ul><li>非监督，线性方法</li><li>通俗理解 就是找出一个最主要的特征，然后进行分析。</li><li>工作原理：
   <ol><li>找出第一个主成分的方向，也就是数据 <code class="prettyprint">方差最大</code> 的方向。</li><li>找出第二个主成分的方向，也就是数据 <code>方差次大</code> 的方向，并且该方向与第一个主成分方向 <code>正交(orthogonal 如果是二维空间就叫垂直)</code>。</li><li>通过这种方式计算出所有的主成分方向。</li><li>通过数据集的协方差矩阵及其特征值分析，我们就可以得到这些主成分的值。</li><li>一旦得到了协方差矩阵的特征值和特征向量，我们就可以保留最大的 N 个特征。这些特征向量也给出了 N 个最重要特征的真实结构，我们就可以通过将数据乘上这 N 个特征向量 从而将它转换到新的空间上。</li><li>为什么正交？ 
     <ul><li>正交是为了数据有效性损失最小</li><li>正交的一个原因是特征值的特征向量是正交的</li><li></ul></li></ol></li></ul> 
 <ul><li>优点：降低数据的复杂性，识别最重要的多个特征。</li><li>缺点：不一定需要，且可能损失有用信息。</li><li>适用数据类型：数值型数据。</li><li>学习资料 
   <ul><li><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#">PCA</a> </li><li><a href="https://www.matongxue.com/madocs/1025/">如何理解主成分分析</a></li></ul></li></ul> 
 <h4>LDA(Linear Discriminant Analysis, 线性判别分析)</h4> 
 <ul><li>监督，线性方法</li><li>主要思想： 给定训练集样本，设法将样本投影到一条直线上，使得同类样本到投影点尽可能近，异类样本到投影点尽可能远；在对新样本进行分类时，将其投影到同样的直线上，再根据投影点的位置来确定新样本的分类。</li></ul> 
 <h4>TSNE(T-Distributed Stochastic Neighbour Embedding, T分布和随机近邻嵌入)</h4> 
 <ul><li>非线性，给予概率的方法</li></ul> 
</article>
                </div>
        